{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 분석 기법 - Vector space model </h1>\n",
    "- 문서를 bag-of-model로 표현해 보자.\n",
    "- 표현된 bag-of-model로 간단한 머신러닝 알고리즘을 적용해보자\n",
    "- TF-IDF를 통해 중요한 단어를 추출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kkma package\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "news1 = \"\"\"\n",
    "중국이 관영매체를 통해 미국 공화당 대선후보인 도널드 트럼프가 떨어지는 지지도를 만회하기 위해 '중국 때리기 카드'를 쓰고 있다고 반박에 나섰다.\n",
    "관영 신화통신은 9일 논평을 통해 트럼프의 선동적인 수사는 '미국 중서부 블루칼라 유권자에게 어필하려는 의도'라며 \"위험하고 미·중 양국관계에 아무런 실질적 도움이 안 된다\"고 주장했다.\n",
    "트럼프 후보는 전날 미국 디트로이트에서 경제공약을 발표하면서 \"중국이 미국과 교역에서 상상할 수 있는 모든 방법으로 기존의 룰을 깨고 있고 미국 재정적자의 절반가량은 중국의 책임\"이라고 주장했다.\n",
    "트럼프는 이어 중국이 불법적으로 수출 보조금을 지급하고 위안화 환율을 조작하고 있으며 지적재산권을 훔치고 있다고 비난을 퍼부었다.\n",
    "그는 중국의 수출품을 차단하고 세계 무역규정을 재협상함으로써 미국경제를 부흥시켜야 한다며 \"글로벌리즘이 아닌 아메리카니즘(Americanism)이 우리의 신조가 될 것\"이라고 강조했다.\n",
    "신화통신은 이에 대해 \"트럼프와 그의 팀이 미국의 경제 부진에 대해 중국과 세계 자유무역을 희생양으로 삼음으로써 자유무역주의라는 공화당의 전통 정신을 위배하고 있다\"고 반박했다.\n",
    "그러면서 미국 정치인들의 '중국 때리기'는 미국 자체의 구조적 퇴보를 숨기려 노상 행해졌던 일이라고 지적했다. 신화통신은 이어 미국이 이런 문제를 해결하려면 뼈를 깎는 구조조정이 필요하다고 조언하기도 했다.\n",
    "신화통신은 또 미국 상무부의 통계를 인용해 트럼프식 접근법으로는 미국 내 350만 개 일자리가 희생될 것이고 경제침체와 함께 미국 소비자들이 더 비싸게 물건을 사야 하는 결과를 초래할 뿐이라고 강조했다.\n",
    "지난해 대미 무역수지 흑자만 3천700억 달러를 낸 중국은 트럼프 후보뿐 힐러리 클린턴 민주당 대선 후보도 보호무역 강화 기조의 선거공약을 내놓자 이들 후보의 공약이 대선 후 현실화될지에 촉각을 곤두세우고 있다.\n",
    "AP통신은 이날 신화통신의 반박 논평 소식을 전하며 선거유세 기간 트럼프 후보의 중국에 대한 비판이 공산당 일당독재의 정치체제나 인권 남용에 대한 언급은 거의 없이 중국의 무역관행에만 초점을 맞추고 있다고 지적했다.\n",
    "\"\"\"\n",
    "\n",
    "# 명사만 추출해보자 - \n",
    "kkma = Kkma()\n",
    "news1_pos = kkma.pos(news1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk # 형태소 분석할 떄 데이터 탐색할 때 사용하면 편리\n",
    "text = nltk.Text(news1_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535\n",
      "264\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'news1_nouns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cfe27ba26131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# returns number of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# returns number of unique tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews1_nouns\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# returns frequency distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'news1_nouns' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(text.tokens))                 # returns number of tokens\n",
    "print(len(set(text.tokens)))            # returns number of unique tokens\n",
    "pprint(news1_nouns)    # returns frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> kkma package vs twitter package</h3>\n",
    "- kkma의 경우 우리가 알고 있는 품사보다 형태소를 더 작은 단위로 분석 (연구용)\n",
    "- 배경지식이 없다면 해석하기 힘듬\n",
    "- 트위터 다니시다가 만든 새로운 형태소 분석 패키지 : twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 대통령 취임사로 tf vector 만들어보기\n",
    "\n",
    "president_address = open('대통령취임사.2txt.txt','r').read()\n",
    "# print(president_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['국민/Noun',\n",
      " '여러분/Noun',\n",
      " '만/Noun',\n",
      " '해외동포/Noun',\n",
      " '여러분/Noun',\n",
      " '저/Noun',\n",
      " '오늘/Noun',\n",
      " '대한민국/Noun',\n",
      " '제/Noun',\n",
      " '대통령/Noun']\n"
     ]
    }
   ],
   "source": [
    "# Twitter package\n",
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()\n",
    "pos_address = ['/'.join(t) for t in pos_tagger.pos(president_address) if t[1]=='Noun'] # 형태소로 tokenizing // 동음이의어를 구분하기 좋음\n",
    "#pos_news = ['/'.join(t) for t in pos_tagger.pos(news1) if t[1]=='Noun']\n",
    "\n",
    "# list 원소 중 10개를 살펴보기\n",
    "pprint(pos_address[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n",
      "406\n",
      "[('국민/Noun', 57),\n",
      " ('것/Noun', 33),\n",
      " ('수/Noun', 28),\n",
      " ('우리/Noun', 23),\n",
      " ('시대/Noun', 22),\n",
      " ('여러분/Noun', 19),\n",
      " ('문화/Noun', 18),\n",
      " ('저/Noun', 16),\n",
      " ('행복/Noun', 15),\n",
      " ('대한민국/Noun', 12),\n",
      " ('국가/Noun', 12),\n",
      " ('새/Noun', 11),\n",
      " ('사회/Noun', 10),\n",
      " ('경제/Noun', 9),\n",
      " ('희망/Noun', 9),\n",
      " ('창조경제/Noun', 8),\n",
      " ('길/Noun', 8),\n",
      " ('세계/Noun', 8),\n",
      " ('신뢰/Noun', 8),\n",
      " ('정부/Noun', 8)]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "# Text() - 단일 문서에 대한 탐색에 좋은 함수가 내장\n",
    "text = nltk.Text(pos_address) # 형태소 분석이 된 list를 input으로 넣어주면\n",
    "\n",
    "\n",
    "print(len(text.tokens)) # 문서에서 나온 token 수\n",
    "print(len(set(text.tokens))) # 겹치지 않는 token 개수\n",
    "pprint(text.vocab().most_common(20)) # 가장 빈번히 나온 token 나열\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 빈도 기준으로 token plot - distribution확인\n",
    "%matplotlib inline \n",
    "text.plot(50)\n",
    "\n",
    "# font 문제가 있다면 먼저 실행\n",
    "from matplotlib import font_manager, rc\n",
    "font_fname = 'c:/windows/fonts/NanumGothicLight.ttf'     # A font of your choice\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FreqDist 함수로도 같은 기능을 수행할 수 있다.\n",
    "freq = nltk.FreqDist(text)\n",
    "freq['것/Noun']\n",
    "freq.most_common(10)\n",
    "freq.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TF-IDF 벡터 만들기</h3>\n",
    "- TF 벡터와 TF-IDF 벡터는 scikit learn(http://scikit-learn.org/)을 통해 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 한글 bag-of-words 만들어보자\n",
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "\n",
    "# 연합뉴스 로딩\n",
    "kor_corpus = pd.read_csv(r'D:\\workspace_python\\한류_연합뉴스.csv', encoding='cp949')\n",
    "\n",
    "# 뉴스기사 정제\n",
    "\n",
    "pos_tagger = Twitter()\n",
    "#pos_address = ['/'.join(t) for t in pos_tagger.pos(news) if t[1]=='Noun' for news in kor_corpus['contents']] # 형태소로 tokenizing // 동음이의어를 구분하기 좋음\n",
    "\n",
    "\n",
    "# list 원소 중 10개를 살펴보기\n",
    "#pprint(pos_address[:10])\n",
    "total_news = []\n",
    "\n",
    "# 한글 문서 명사만 뽑아서 tf-idf\n",
    "stopword = ['것','수','저']\n",
    "for news in kor_corpus['contents'].head():\n",
    "    pos_news = ['/'.join(t[:-1]) for t in pos_tagger.pos(news) if ((t[1]=='Noun') & (t[0] not in stopword))]\n",
    "    total_news.append(' '.join(pos_news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "kor_vectorizer = CountVectorizer(min_df=1) # 등장하는 단어들에 대한 오브젝트\n",
    "kor_bow = kor_vectorizer.fit_transform(total_news) # 딕셔너리에 실제 단어들을 입력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가수', '가요', '각각', '개정', '개척', '거점', '건강', '검진', '견실', '견지해', '결정', '경북', '경연', '경우', '경운대', '경쟁', '경제', '경향', '계획', '고고', '고신대', '공간', '공공', '공연장', '공유', '과거', '관계자', '관련', '광부', '광주', '교류', '교육', '교육감', '구미', '구미시장', '구축', '국내', '국립병원', '국민', '국악', '국제', '권정열', '규모', '그룹', '극장', '극히', '근무', '금은', '기기', '기도', '기사', '기술', '기업', '기자', '기존', '기타', '기획', '김영만', '김태우', '끼리', '나라', '날로', '내년', '내달', '내용', '노래', '노아', '누구', '눈길', '능이', '다음', '다이어트', '단일', '당일', '대구', '대부분', '대상', '대중', '대표', '대학', '대한', '대해', '댄스', '더욱', '도시', '도움', '도지사', '독소', '동상', '드림', '등록', '디자이너', '디자인', '라며', '라피', '리밍', '만원', '메시지', '몽골', '문의', '문전', '문화', '물리', '미래', '미사일', '미용', '민간', '박대', '박순', '박창수', '반복', '반응', '발굴', '발급', '발생', '발전', '방어', '배너', '배치', '번역', '베트남', '병원', '보도', '보유', '복부', '복음병원', '본문', '본선', '봉사', '부문', '부분', '부산', '분야', '분위기', '분해', '브랜드', '브리핑', '사드', '사립', '사무국', '사업', '사흘', '산업', '상의', '상표', '생각', '생산', '생소', '서로', '서울', '석션기', '선도', '선발', '설명', '성공', '성분', '성인', '세계', '세라믹', '세종', '센터', '셀룰라이트', '수도', '수료증', '수술', '수여', '순환', '스타', '시각', '시간', '시민운동', '시스템', '시작', '시장', '시한', '식품', '신뢰', '신제품', '신청', '신체', '실장', '실제', '실패', '십센치', '아스타나', '아트', '안목', '알마티', '앰플', '약간', '양국', '억원', '언론', '업계', '업체', '에서', '에스테', '여부', '여성가족부', '여운', '여제', '여종', '여파', '연매출', '연합뉴스', '열기', '열석', '열정', '영어', '영역', '예선', '예술', '오일', '오해', '온돌방', '올해', '외국', '외국어', '외래', '외부', '외적인', '요소', '요인', '우려', '우리', '우리나라', '원액', '원적외선', '원정', '위축', '위해', '윈윈할', '유통', '윤태용', '은상', '음이온', '의료', '의복', '의장', '이문열', '이상', '이어령', '이한수', '이후', '인문학', '인터뷰', '일반', '일부', '일비', '일정', '입원', '자본', '자산', '자원봉사', '자원봉사자', '잠깐', '장관', '장점', '재생산', '적극', '적지', '전남', '전문', '전체', '전통', '접목', '정부', '정상', '제동', '제조', '제품', '제품군', '조금', '주민', '중국', '중소기업', '중이', '중증', '지급', '지난해', '지방', '지성', '지역', '지원', '지향', '직접', '진료', '진출', '진흥', '질환', '차례', '착안', '창의성', '천만원', '청소년', '체계', '체육관', '초기', '총상', '최근', '최재천', '추물', '출시', '출신', '취급', '취소', '취재', '취지', '측면', '치료', '치열', '카자흐스탄', '카페인', '캐비어', '코팅', '콘서트', '콘텐츠', '크림', '태반', '통해', '투자', '트로트', '특허', '티셔츠', '파견', '파트너', '팔공홀', '팝핀', '패션', '패션쇼', '페스티벌', '프랑스', '프로그램', '피부', '학교', '한국', '한류', '함유', '항상', '행사', '헬스시티', '현재', '현지', '협력', '협의', '혜택', '호응', '홈페이지', '화장품', '활성화', '회관', '회사', '회장', '효과']\n",
      "(5, 354)\n",
      "[[0 0 0 ..., 1 0 1]\n",
      " [1 2 1 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(kor_vectorizer.get_feature_names())\n",
    "# bag-of-words\n",
    "print(kor_bow.shape)\n",
    "print(kor_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer() # tfidf 변환 오브젝트 생성\n",
    "tfidf = transformer.fit_transform(kor_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.06236855,\n",
       "         0.        ,  0.06236855],\n",
       "       [ 0.07572825,  0.1514565 ,  0.06109707, ...,  0.        ,\n",
       "         0.07572825,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.05084439, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대구\n",
      "패션\n",
      "문화\n",
      "인문학\n",
      "극장\n",
      "패션쇼\n",
      "회관\n",
      "차례\n",
      "한국\n",
      "예술\n",
      "행사\n",
      "최재천\n",
      "티셔츠\n",
      "시간\n",
      "취지\n",
      "지성\n",
      "출신\n",
      "국제\n",
      "기획\n",
      "이문열\n"
     ]
    }
   ],
   "source": [
    "# tf idf를 이용한 중요 단어 순위\n",
    "import numpy as np\n",
    "\n",
    "doc_index = 3\n",
    "\n",
    "arr = np.array(tfidf.toarray()[doc_index])\n",
    "idx = (-arr).argsort()[:20]\n",
    "\n",
    "for i in idx:\n",
    "    print(kor_vectorizer.get_feature_names()[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fta', '김치', '미국', '한국']\n",
      "[[0 1 0 1]\n",
      " [2 0 1 1]\n",
      " [0 0 1 1]]\n",
      "[[ 0.          0.861037    0.          0.50854232]\n",
      " [ 0.90100815  0.          0.34261996  0.26607496]\n",
      " [ 0.          0.          0.78980693  0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "# 간단한 예제\n",
    "\n",
    "ex_text = [\"한국 김치 \",\n",
    "          \"한국 미국 FTA FTA \",\n",
    "          \"한국 미국 \"]\n",
    "\n",
    "kor_vectorizer = CountVectorizer(min_df=1)\n",
    "kor_bow = kor_vectorizer.fit_transform(ex_text)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(kor_bow.toarray())\n",
    "print(kor_vectorizer.get_feature_names())\n",
    "print(kor_bow.toarray())\n",
    "print(tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> 문서 비교 </h3>\n",
    "- 각각의 문서를 bag-of-words를 이용해 수치형 벡터로 표현\n",
    "- 문서끼리 얼마나 가까운지 similarity를 계산하여 비교해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "                       김영만 기자 한국 고고 미사일 방어 체계 사드 \n"
     ]
    }
   ],
   "source": [
    "nltk.Text(pos_news).concordance('김영만', width=50, lines=5)\n",
    "nltk.Text(pos_news).dispersion_plot(['한국','미사일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['세종', '연합뉴스', '김영만', '기자', '한국', '고고', '미사일', '방어', '체계', '사드', '배치', '결정', '이후', '중국', '내', '한류', '위축', '우려', '대해', '문화', '체육관', '광부', '윤태용', '문화', '콘텐츠', '산업', '실장', '일', '중국', '우리', '우리나라', '문화', '콘텐츠', '더욱', '중국', '고', '윤', '실장', '날', '연합뉴스', '인터뷰', '중국', '한류', '진출', '및', '성공', '여부', '문화', '외적인', '요소', '콘텐츠', '생산', '유통', '며', '그', '한국', '창의성', '기술', '중국', '자본', '시장', '각각', '장점', '중이', '서로', '협력', '윈윈할', '라며', '한국', '우리', '항상', '중국', '협력', '중', '시한', '메시지', '중국', '측', '게', '고', '전체', '기사', '본문', '배너', '두', '나라', '외부', '요인', '약간', '오해', '잠깐', '문화', '경제', '측면', '안목', '미래', '지향', '시각', '견지해', '한국', '중국', '언론', '중국', '한류', '제동', '관련', '보도', '반복', '재생산', '대해', '그', '실제', '과', '측면', '적지', '중', '문화', '예술', '교류', '신뢰', '국민', '업계', '우려', '고', '그', '또', '최근', '사드', '여파', '양국', '분위기', '속', '극히', '일부', '한류', '행사', '일정', '조금', '대부분', '취소', '정상', '알', '며', '며', '국내', '언론', '끼리', '한국', '중국', '언론', '식', '취재', '경쟁', '건', '양국', '발전', '도움', '안', '고', '윤', '실장', '문화', '예술', '분야', '공공', '부문', '선도', '중국', '측', '적극', '협력', '함', '사드', '여파', '위축', '민간', '부문', '대중', '한류', '활성화', '고']\n"
     ]
    }
   ],
   "source": [
    "print(pos_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 연어(collocation) 찾기</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collocations among tagged words:\n",
      "[(('21', 'NR'), ('세기', 'NNG')),\n",
      " (('5', 'NR'), ('천', 'NR')),\n",
      " (('·', 'SP'), ('무형', 'NNG')),\n",
      " (('각종', 'NNG'), ('불공정', 'NNG')),\n",
      " (('경축', 'NNG'), ('사절', 'NNG')),\n",
      " (('계와', 'MAG'), ('품앗이', 'NNG')),\n",
      " (('그리고', 'MAG'), ('이러하', 'VA')),\n",
      " (('근심', 'NNG'), ('없이', 'MAG')),\n",
      " (('글로', 'MAG'), ('벌', 'VV')),\n",
      " (('남북', 'NNG'), ('간', 'NNB'))]\n",
      "\n",
      "Collocations among words:\n",
      "[('과학', '기술'),\n",
      " ('문화', '융성'),\n",
      " ('경제', '부흥'),\n",
      " ('창조', '경제'),\n",
      " ('국가', '발전'),\n",
      " ('바라', 'ㅂ니다'),\n",
      " ('국민', '여러분'),\n",
      " ('국민', '여러'),\n",
      " ('행복', '시대'),\n",
      " ('우리', '대한민국')]\n",
      "\n",
      "Collocations among tags:\n",
      "[('XR', 'XSA'), ('NR', 'NNM'), ('JC', 'OL'), ('MAC', 'SS'), ('VXV', 'EPH')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from konlpy.utils import pprint\n",
    "from nltk import collocations\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "measures = collocations.BigramAssocMeasures()\n",
    "#doc = kolaw.open('constitution.txt').read()\n",
    "doc = open('대통령취임사.txt','r').read()\n",
    "print('\\nCollocations among tagged words:')\n",
    "\n",
    "tagged_words = Kkma().pos(doc)\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "pprint(finder.nbest(measures.pmi, 10)) # top 5 n-grams with highest PMI\n",
    "\n",
    "print('\\nCollocations among words:')\n",
    "words = [w for w, t in tagged_words]\n",
    "ignored_words = [u'안녕']\n",
    "finder = collocations.BigramCollocationFinder.from_words(words)\n",
    "\n",
    "finder.apply_word_filter(lambda w: len(w) < 2 or w in ignored_words)\n",
    "finder.apply_freq_filter(3) # only bigrams that appear 3+ times\n",
    "pprint(finder.nbest(measures.pmi, 10))\n",
    "\n",
    "print('\\nCollocations among tags:')\n",
    "tags = [t for w, t in tagged_words]\n",
    "finder = collocations.BigramCollocationFinder.from_words(tags)\n",
    "pprint(finder.nbest(measures.pmi, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collocations among tagged words:\n",
      "[(('5', 'NR'), ('천', 'NR'), ('년', 'NNB')),\n",
      " (('년', 'NNB'), ('유', 'NNG'), ('·', 'SP')),\n",
      " (('드립', 'NNG'), ('니', 'VV'), ('다', 'ECS')),\n",
      " (('밤새', 'NNG'), ('불이', 'NNG'), ('꺼지', 'VV')),\n",
      " (('부탁', 'NNG'), ('드립', 'NNG'), ('니', 'VV')),\n",
      " (('여성', 'NNG'), ('이나', 'JC'), ('장애인', 'NNG')),\n",
      " (('영하', 'NNG'), ('수십', 'NR'), ('도', 'NNM')),\n",
      " (('유', 'NNG'), ('·', 'SP'), ('무형', 'NNG')),\n",
      " (('이나', 'JC'), ('장애인', 'NNG'), ('또는', 'MAG')),\n",
      " (('이상', 'NNG'), ('핵과', 'NNG'), ('미사일', 'NNG'))]\n",
      "\n",
      "Collocations among words:\n",
      "[('국민', '행복', '시대')]\n",
      "\n",
      "Collocations among tags:\n",
      "[('VXV', 'EPH', 'ETN'),\n",
      " ('XR', 'XSA', 'ETN'),\n",
      " ('SP', 'MAC', 'SS'),\n",
      " ('SS', 'SP', 'MAC'),\n",
      " ('ECS', 'VXV', 'EPH')]\n"
     ]
    }
   ],
   "source": [
    "measures = collocations.TrigramAssocMeasures()\n",
    "#doc = kolaw.open('constitution.txt').read()\n",
    "doc = open('대통령취임사.txt','r').read()\n",
    "print('\\nCollocations among tagged words:')\n",
    "\n",
    "tagged_words = Kkma().pos(doc)\n",
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "pprint(finder.nbest(measures.pmi, 10)) # top 5 n-grams with highest PMI\n",
    "\n",
    "print('\\nCollocations among words:')\n",
    "words = [w for w, t in tagged_words]\n",
    "ignored_words = [u'안녕']\n",
    "finder = collocations.TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "finder.apply_word_filter(lambda w: len(w) < 2 or w in ignored_words)\n",
    "finder.apply_freq_filter(3) # only bigrams that appear 3+ times\n",
    "pprint(finder.nbest(measures.pmi, 10))\n",
    "\n",
    "print('\\nCollocations among tags:')\n",
    "tags = [t for w, t in tagged_words]\n",
    "finder = collocations.TrigramCollocationFinder.from_words(tags)\n",
    "pprint(finder.nbest(measures.pmi, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
